{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data QA Case: ``mggg-states``\n",
    "========================\n",
    "\n",
    "Below are the steps involved in performing automated data quality checks on ``mggg-states`` data. \n",
    "\n",
    "*Note:* the automated checks are not completely exhaustive and further manual checks are required.\n",
    "\n",
    "Step 0. Setup\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install numpy\n",
    "# !pip3 install pandas\n",
    "# !pip3 install geopandas\n",
    "# !pip3 install wikipedia\n",
    "\n",
    "# !pip3 install git+https://github.com/KeiferC/gdutils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import wikipedia\n",
    "import os\n",
    "\n",
    "import gdutils.datamine as dm\n",
    "import gdutils.dataqa as dq\n",
    "import gdutils.extract as et"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Data collection\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1.1.__ Gather ``mggg-states`` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm.clone_gh_repos(account='mggg-states', account_type='orgs', outpath=os.path.join('qafiles', 'mggg'))\n",
    "#     # this will take some time to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mggg_gdfs = {}\n",
    "\n",
    "for filepath in dm.list_files_of_type('.zip', os.path.join('qafiles', 'mggg')):\n",
    "      mggg_gdfs[os.path.basename(filepath)[:-4]] = et.read_file(filepath).extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1.3.__ Gather MEDSL data for comparison purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:27} : {}'.format('Repo Name', 'Repo URL'))\n",
    "print('------------------------------------------------------------------')\n",
    "for (repo, url) in dm.list_gh_repos(account='MEDSL', account_type='orgs'):\n",
    "    print(\"{:27} : {}\".format(repo, url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medsl_repos = ['official-precinct-returns', # precinct-level 2016 election results\n",
    "               '2018-elections-official'] # constituency-level 2018 election results\n",
    "    \n",
    "# dm.clone_gh_repos(account='MEDSL', account_type='orgs', repos=medsl_repos, outpath=os.path.join('qafiles', 'medsl'))\n",
    "#     # this will take some time to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medsl_dfs = {}\n",
    "\n",
    "# Note: will likely throw warnings because MEDSL data contains multiple datatypes in some columns\n",
    "for filepath in dm.list_files_of_type('.zip', os.path.join('qafiles', 'medsl')):\n",
    "    medsl_dfs[os.path.basename(filepath)[:-4]] = et.read_file(filepath).extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medsl_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1.2.__ Gather Wikipedia data for comparison purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate wikipedia page titles\n",
    "states = ['Alabama', 'Alaska','Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', \n",
    "          'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', \n",
    "          'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', \n",
    "          'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', \n",
    "          'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', \n",
    "          'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', \n",
    "          'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
    "\n",
    "wiki_pres_elections = ['United States presidential election']\n",
    "wiki_fed_elections  = ['United States Senate election']\n",
    "                      #'United States House of Representatives election']\n",
    "election_years_to_check = [2016, 2018]\n",
    "\n",
    "wiki_titles = []\n",
    "for yr in election_years_to_check:\n",
    "    generate_title = lambda yr, etype: str(yr) + ' ' + etype \n",
    "    \n",
    "    if yr % 4 == 0:\n",
    "        wiki_titles.append(generate_title(yr, wiki_pres_elections[0]))\n",
    "        \n",
    "    [wiki_titles.append(generate_title(yr, etype) + ' in ' + st) \n",
    "         for etype in wiki_fed_elections\n",
    "         for st in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather wikipedia page URLs\n",
    "wiki_urls = {}\n",
    "for page_title in wiki_titles:\n",
    "    try:\n",
    "        wiki_urls[page_title] = wikipedia.page(title=page_title).url\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print retrieved page URLs\n",
    "for wiki in wiki_urls:\n",
    "    print('{:65}\\n\\t{}'.format(wiki, wiki_urls[wiki]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather wikipedia tabular election results\n",
    "wiki_tables = {}\n",
    "for wiki in wiki_urls:\n",
    "    try:\n",
    "        wiki_tables[wiki] = pd.read_html(wiki_urls[wiki])\n",
    "    except Exception as e:\n",
    "        print(\"Unable to gather Wikipedia tabular data:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display wikipedia tabular election data\n",
    "for wiki in wiki_tables:\n",
    "    print('================================================')\n",
    "    print('Wiki: {} '.format(wiki))\n",
    "    print('================================================')\n",
    "    \n",
    "    for i in range(len(wiki_tables[wiki])):\n",
    "        print('TABLE {}: ############################\\n{}\\n\\n\\n'.format(i, wiki_tables[wiki][i].head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually gather found tabular election results\n",
    "\n",
    "# Generate shorthand keys for wikipedia tabular data\n",
    "pres_election_short = ['PRES']\n",
    "fed_elections_short = ['SEN'] # , 'USH']\n",
    "st_abvs = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', \n",
    "           'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', \n",
    "           'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "wiki_titles_short = []\n",
    "for yr in election_years_to_check:\n",
    "    generate_title = lambda st_abv, yr, etype: etype + str(yr % 100) + '_' + st_abv\n",
    "    \n",
    "    if yr % 4 == 0:\n",
    "        [wiki_titles_short.append(generate_title(abv, yr, pres_election_short[0])) \n",
    "            for abv in st_abvs]\n",
    "        \n",
    "    [wiki_titles_short.append(generate_title(abv, yr, etype)) \n",
    "         for etype in fed_elections_short\n",
    "         for abv in st_abvs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dfs = {}\n",
    "\n",
    "# wiki_pres16_by_state = [{state_abv : table} for _______] # TODO\n",
    "\n",
    "# has to be done manually because every wikipedia page is different\n",
    "\n",
    "# wiki_dfs['PRES16_AL'] = wiki_pres16_by_state['AL']\n",
    "# wiki_dfs['PRES16_AK'] = wiki_pres16_by_state['AK']\n",
    "# wiki_dfs['PRES16_AZ'] = wiki_pres16_by_state['AZ']\n",
    "# wiki_dfs['PRES16_AR'] = wiki_pres16_by_state['AR']\n",
    "# wiki_dfs['PRES16_CA'] = wiki_pres16_by_state['CA']\n",
    "# wiki_dfs['PRES16_CO'] = wiki_pres16_by_state['CO']\n",
    "# wiki_dfs['PRES16_CT'] = wiki_pres16_by_state['CT']\n",
    "# wiki_dfs['PRES16_DE'] = wiki_pres16_by_state['DE']\n",
    "# wiki_dfs['PRES16_FL'] = wiki_pres16_by_state['FL']\n",
    "# wiki_dfs['PRES16_GA'] = wiki_pres16_by_state['GA']\n",
    "# wiki_dfs['PRES16_HI'] = wiki_pres16_by_state['HI']\n",
    "# wiki_dfs['PRES16_ID'] = wiki_pres16_by_state['ID']\n",
    "# wiki_dfs['PRES16_IL'] = wiki_pres16_by_state['IL']\n",
    "# wiki_dfs['PRES16_IN'] = wiki_pres16_by_state['IN']\n",
    "# wiki_dfs['PRES16_IA'] = wiki_pres16_by_state['IA']\n",
    "# wiki_dfs['PRES16_KS'] = wiki_pres16_by_state['KS']\n",
    "# wiki_dfs['PRES16_KY'] = wiki_pres16_by_state['KY']\n",
    "# wiki_dfs['PRES16_LA'] = wiki_pres16_by_state['LA']\n",
    "# wiki_dfs['PRES16_ME'] = wiki_pres16_by_state['ME']\n",
    "# wiki_dfs['PRES16_MD'] = wiki_pres16_by_state['MD']\n",
    "# wiki_dfs['PRES16_MA'] = wiki_pres16_by_state['MA']\n",
    "# wiki_dfs['PRES16_MI'] = wiki_pres16_by_state['MI']\n",
    "# wiki_dfs['PRES16_MN'] = wiki_pres16_by_state['MN']\n",
    "# wiki_dfs['PRES16_MS'] = wiki_pres16_by_state['MS']\n",
    "# wiki_dfs['PRES16_MO'] = wiki_pres16_by_state['MO']\n",
    "# wiki_dfs['PRES16_MT'] = wiki_pres16_by_state['MT']\n",
    "# wiki_dfs['PRES16_NE'] = wiki_pres16_by_state['NE']\n",
    "# wiki_dfs['PRES16_NV'] = wiki_pres16_by_state['NV']\n",
    "# wiki_dfs['PRES16_NH'] = wiki_pres16_by_state['NH']\n",
    "# wiki_dfs['PRES16_NJ'] = wiki_pres16_by_state['NJ']\n",
    "# wiki_dfs['PRES16_NM'] = wiki_pres16_by_state['NM']\n",
    "# wiki_dfs['PRES16_NY'] = wiki_pres16_by_state['NY']\n",
    "# wiki_dfs['PRES16_NC'] = wiki_pres16_by_state['NC']\n",
    "# wiki_dfs['PRES16_ND'] = wiki_pres16_by_state['ND']\n",
    "# wiki_dfs['PRES16_OH'] = wiki_pres16_by_state['OH']\n",
    "# wiki_dfs['PRES16_OK'] = wiki_pres16_by_state['OK']\n",
    "# wiki_dfs['PRES16_OR'] = wiki_pres16_by_state['OR']\n",
    "# wiki_dfs['PRES16_PA'] = wiki_pres16_by_state['PA']\n",
    "# wiki_dfs['PRES16_RI'] = wiki_pres16_by_state['RI']\n",
    "# wiki_dfs['PRES16_SC'] = wiki_pres16_by_state['SC']\n",
    "# wiki_dfs['PRES16_SD'] = wiki_pres16_by_state['SD']\n",
    "# wiki_dfs['PRES16_TN'] = wiki_pres16_by_state['TN']\n",
    "# wiki_dfs['PRES16_TX'] = wiki_pres16_by_state['TX']\n",
    "# wiki_dfs['PRES16_UT'] = wiki_pres16_by_state['UT']\n",
    "# wiki_dfs['PRES16_VT'] = wiki_pres16_by_state['VT']\n",
    "# wiki_dfs['PRES16_VA'] = wiki_pres16_by_state['VA']\n",
    "# wiki_dfs['PRES16_WA'] = wiki_pres16_by_state['WA']\n",
    "# wiki_dfs['PRES16_WV'] = wiki_pres16_by_state['WV']\n",
    "# wiki_dfs['PRES16_WI'] = wiki_pres16_by_state['WI']\n",
    "# wiki_dfs['PRES16_WY'] = wiki_pres16_by_state['WY']\n",
    "\n",
    "wiki_dfs['SEN16_AL'] = wiki_tables['2016 United States Senate election in Alabama'][19]\n",
    "wiki_dfs['SEN16_AK'] = wiki_tables['2016 United States Senate election in Alaska'][20]\n",
    "wiki_dfs['SEN16_AZ'] = wiki_tables['2016 United States Senate election in Arizona'][40]\n",
    "# wiki_dfs['SEN16_AR'] =\n",
    "# wiki_dfs['SEN16_CA'] =\n",
    "# wiki_dfs['SEN16_CO'] = \n",
    "# wiki_dfs['SEN16_CT'] = \n",
    "# wiki_dfs['SEN16_DE'] = \n",
    "# wiki_dfs['SEN16_FL'] = \n",
    "# wiki_dfs['SEN16_GA'] = \n",
    "# wiki_dfs['SEN16_HI'] = \n",
    "# wiki_dfs['SEN16_ID'] = \n",
    "# wiki_dfs['SEN16_IL'] = \n",
    "# wiki_dfs['SEN16_IN'] = \n",
    "# wiki_dfs['SEN16_IA'] = \n",
    "# wiki_dfs['SEN16_KS'] = \n",
    "# wiki_dfs['SEN16_KY'] = \n",
    "# wiki_dfs['SEN16_LA'] = \n",
    "# wiki_dfs['SEN16_ME'] = \n",
    "# wiki_dfs['SEN16_MD'] = \n",
    "# wiki_dfs['SEN16_MA'] = \n",
    "# wiki_dfs['SEN16_MI'] = \n",
    "# wiki_dfs['SEN16_MN'] = \n",
    "# wiki_dfs['SEN16_MS'] = \n",
    "# wiki_dfs['SEN16_MO'] = \n",
    "# wiki_dfs['SEN16_MT'] = \n",
    "# wiki_dfs['SEN16_NE'] = \n",
    "# wiki_dfs['SEN16_NV'] = \n",
    "# wiki_dfs['SEN16_NH'] = \n",
    "# wiki_dfs['SEN16_NJ'] = \n",
    "# wiki_dfs['SEN16_NM'] = \n",
    "# wiki_dfs['SEN16_NY'] = \n",
    "# wiki_dfs['SEN16_NC'] = \n",
    "# wiki_dfs['SEN16_ND'] = \n",
    "# wiki_dfs['SEN16_OH'] = \n",
    "# wiki_dfs['SEN16_OK'] = \n",
    "# wiki_dfs['SEN16_OR'] = \n",
    "# wiki_dfs['SEN16_PA'] = \n",
    "# wiki_dfs['SEN16_RI'] = \n",
    "# wiki_dfs['SEN16_SC'] = \n",
    "# wiki_dfs['SEN16_SD'] = \n",
    "# wiki_dfs['SEN16_TN'] = \n",
    "# wiki_dfs['SEN16_TX'] = \n",
    "# wiki_dfs['SEN16_UT'] = \n",
    "# wiki_dfs['SEN16_VT'] = \n",
    "# wiki_dfs['SEN16_VA'] = \n",
    "# wiki_dfs['SEN16_WA'] = \n",
    "# wiki_dfs['SEN16_WV'] = \n",
    "# wiki_dfs['SEN16_WI'] = \n",
    "# wiki_dfs['SEN16_WY'] = \n",
    "\n",
    "wiki_dfs['SEN18_AZ'] = wiki_tables['2018 United States Senate election in Arizona'][40]\n",
    "# wiki_dfs['SEN18_AR'] = \n",
    "# wiki_dfs['SEN18_CA'] = \n",
    "# wiki_dfs['SEN18_CO'] = \n",
    "# wiki_dfs['SEN18_CT'] = \n",
    "# wiki_dfs['SEN18_DE'] = \n",
    "# wiki_dfs['SEN18_FL'] = \n",
    "# wiki_dfs['SEN18_GA'] = \n",
    "# wiki_dfs['SEN18_HI'] = \n",
    "# wiki_dfs['SEN18_ID'] = \n",
    "# wiki_dfs['SEN18_IL'] = \n",
    "# wiki_dfs['SEN18_IN'] = \n",
    "# wiki_dfs['SEN18_IA'] = \n",
    "# wiki_dfs['SEN18_KS'] = \n",
    "# wiki_dfs['SEN18_KY'] = \n",
    "# wiki_dfs['SEN18_LA'] = \n",
    "# wiki_dfs['SEN18_ME'] = \n",
    "# wiki_dfs['SEN18_MD'] = \n",
    "# wiki_dfs['SEN18_MA'] = \n",
    "# wiki_dfs['SEN18_MI'] = \n",
    "# wiki_dfs['SEN18_MN'] = \n",
    "# wiki_dfs['SEN18_MS'] = \n",
    "# wiki_dfs['SEN18_MO'] = \n",
    "# wiki_dfs['SEN18_MT'] = \n",
    "# wiki_dfs['SEN18_NE'] = \n",
    "# wiki_dfs['SEN18_NV'] = \n",
    "# wiki_dfs['SEN18_NH'] = \n",
    "# wiki_dfs['SEN18_NJ'] = \n",
    "# wiki_dfs['SEN18_NM'] = \n",
    "# wiki_dfs['SEN18_NY'] = \n",
    "# wiki_dfs['SEN18_NC'] = \n",
    "# wiki_dfs['SEN18_ND'] = \n",
    "# wiki_dfs['SEN18_OH'] = \n",
    "# wiki_dfs['SEN18_OK'] = \n",
    "# wiki_dfs['SEN18_OR'] = \n",
    "# wiki_dfs['SEN18_PA'] = \n",
    "# wiki_dfs['SEN18_RI'] = \n",
    "# wiki_dfs['SEN18_SC'] = \n",
    "# wiki_dfs['SEN18_SD'] = \n",
    "# wiki_dfs['SEN18_TN'] = \n",
    "# wiki_dfs['SEN18_TX'] = \n",
    "# wiki_dfs['SEN18_UT'] = \n",
    "# wiki_dfs['SEN18_VT'] = \n",
    "# wiki_dfs['SEN18_VA'] = \n",
    "# wiki_dfs['SEN18_WA'] = \n",
    "# wiki_dfs['SEN18_WV'] = \n",
    "# wiki_dfs['SEN18_WI'] = \n",
    "# wiki_dfs['SEN18_WY'] = \n",
    "\n",
    "wiki_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Coming Soon] __Step 1.4.__ Gather Ballotpedia data for comparison purposes\n",
    "\n",
    "*Note:* Depending on response to API access, this step may need to be done manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Data wrangling\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2.1.__ Wrangle MEDSL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Data standardization check of ``mggg-states``\n",
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3.1__ Generate naming conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naming_convention.json') as json_file:\n",
    "    standards_raw = json.load(json_file)\n",
    "    \n",
    "offices = dm.get_keys_by_category(standards_raw, 'offices')\n",
    "parties = dm.get_keys_by_category(standards_raw, 'parties')\n",
    "counts = dm.get_keys_by_category(standards_raw, 'counts')\n",
    "others = dm.get_keys_by_category(standards_raw, \n",
    "            ['geographies', 'demographics', 'districts', 'other'])\n",
    "\n",
    "elections = [office + format(year, '02') + party \n",
    "                 for office in offices\n",
    "                 for year in range(0, 21)\n",
    "                 for party in parties \n",
    "                 if not (office == 'PRES' and year % 4 != 0)]\n",
    "\n",
    "counts = [count + format(year, '02') for count in counts \n",
    "                                     for year in range(0, 20)]\n",
    "\n",
    "standards = elections + counts + others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3.2.__ Check ``mggg-states`` data compliance with naming conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_check = {}\n",
    "\n",
    "for gdf in mggg_gdfs:\n",
    "      naming_check[gdf] = dq.compare_column_names(mggg_gdfs[gdf], standards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_columns = {}\n",
    "\n",
    "# Print results\n",
    "for gdf in naming_check:\n",
    "    print('=========================================')\n",
    "    print('Dataset: {}'.format(gdf))\n",
    "    print('=========================================')\n",
    "    \n",
    "    (matches, diffs) = naming_check[gdf]\n",
    "    matched_columns[gdf] = matches\n",
    "    \n",
    "    diffs = list(diffs)\n",
    "    diffs.sort()\n",
    "    \n",
    "    print('Discrepancies from naming convention:')\n",
    "    print(diffs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Compare ``mggg-states`` data with external sources\n",
    "----------------------------------------------------------\n",
    "\n",
    "__Step 4.1.__ Compare against MEDSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4.2.__ Compare against Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4.3.__ Compare against Ballotpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Check topological soundness of ``mggg-states`` data\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "__Step 5.1.__ Check for empty geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6. Cleanup\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove cloned repos\n",
    "# dm.remove_repos('qafiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete output dump directory\n",
    "# !echo y | rm -rf ./qafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uninstall installed python packages\n",
    "# !echo y | pip3 uninstall numpy\n",
    "# !echo y | pip3 uninstall pandas\n",
    "# !echo y | pip3 uninstall geopandas\n",
    "# !echo y | pip3 uninstall wikipedia\n",
    "\n",
    "# !echo y | pip3 uninstall gdutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reset Jupyter Notebook IPython Kernel\n",
    "# from IPython.core.display import HTML\n",
    "# HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "-------------\n",
    "\n",
    "__Data Standardization__\n",
    "\n",
    "- Manually evaluate column naming discrepancies to determine if changes are needed\n",
    "- Manually evaluate column datatypes to determine if changes are needed\n",
    "\n",
    "__Data Comparison__\n",
    "\n",
    "- Manually investigate large differences found through comparing ``mggg-states`` data with external sources (e.g. Are absentee ballots counted? Are the precinct counts accurate? etc.) \n",
    "- For more accurate comparisons, compare ``mggg-states`` data with those in each States' Secretary of State website\n",
    "\n",
    "__Topological Soundness__\n",
    "\n",
    "- Manually examine shapefiles for gaps and overlaps. *Note:* although gaps and overlaps are not necessarily indicators of inaccurate data (because some counties have precinct islands), they do mean that the data cannot be for chain runs. \n",
    "\n",
    "__Data Documentation__\n",
    "\n",
    "- Do the READMEs provide data sources?\n",
    "- Do the READMEs describe what aggregation/disaggregation processes were used?\n",
    "- Do the READMEs discuss discrepancies/caveats in the data?\n",
    "- Do the READMEs provide scripts used and/or discuss the data wrangling/processing process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
