{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data QA Case: ``mggg-states``\n",
    "========================\n",
    "\n",
    "Below are the steps involved in performing automated data quality checks on ``mggg-states`` data. \n",
    "\n",
    "*Note:* the automated checks are not completely exhaustive and further manual checks are required.\n",
    "\n",
    "Step 0. Setup\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install numpy\n",
    "# !pip3 install pandas\n",
    "# !pip3 install geopandas\n",
    "# !pip3 install wikipedia\n",
    "\n",
    "# !pip3 install git+https://github.com/KeiferC/gdutils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import wikipedia\n",
    "import os\n",
    "\n",
    "import gdutils.datamine as dm\n",
    "import gdutils.dataqa as dq\n",
    "import gdutils.extract as et\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Data collection\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names = [\n",
    "    'Alabama', 'Alaska','Arizona', 'Arkansas', 'California', \n",
    "    'Colorado', 'Connecticut', 'Delaware',  'Florida', 'Georgia', \n",
    "    'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', \n",
    "    'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', \n",
    "    'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', \n",
    "    'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', \n",
    "    'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', \n",
    "    'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', \n",
    "    'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', \n",
    "    'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
    "state_abbreviations = [\n",
    "    'AL', 'AK', 'AZ', 'AR', 'CA', \n",
    "    'CO', 'CT', 'DE', 'FL', 'GA', \n",
    "    'HI', 'ID', 'IL', 'IN', 'IA', \n",
    "    'KS', 'KY', 'LA', 'ME', 'MD', \n",
    "    'MA', 'MI', 'MN', 'MS', 'MO', \n",
    "    'MT', 'NE', 'NV', 'NH', 'NJ', \n",
    "    'NM', 'NY', 'NC', 'ND', 'OH', \n",
    "    'OK', 'OR', 'PA', 'RI', 'SC', \n",
    "    'SD', 'TN', 'TX', 'UT', 'VT', \n",
    "    'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "states = list(zip(state_names, state_abbreviations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1.1.__ Gather ``mggg-states`` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm.clone_gh_repos(account='mggg-states', account_type='orgs', \n",
    "#                   outpath=os.path.join('qafiles', 'mggg'))\n",
    "#     # this will take some time to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mggg_gdfs = {}\n",
    "\n",
    "for filepath in dm.list_files_of_type('.zip', os.path.join('qafiles', 'mggg')):\n",
    "      mggg_gdfs[os.path.basename(filepath)[:-4]] = et.read_file(filepath).extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1.2.__ Gather MEDSL data for comparison purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:27} : {}'.format('Repo Name', 'Repo URL'))\n",
    "print('------------------------------------------------------------------')\n",
    "for (repo, url) in dm.list_gh_repos(account='MEDSL', account_type='orgs'):\n",
    "    print(\"{:27} : {}\".format(repo, url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medsl_repos = ['official-precinct-returns', # precinct-level 2016 election results\n",
    "               '2018-elections-official'] # constituency-level 2018 election results\n",
    "    \n",
    "# dm.clone_gh_repos(account='MEDSL', account_type='orgs', repos=medsl_repos, \n",
    "#                   outpath=os.path.join('qafiles', 'medsl'))\n",
    "#     # this will take some time to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medsl_dfs = {}\n",
    "\n",
    "for filepath in dm.list_files_of_type('.zip', os.path.join('qafiles', 'medsl')):\n",
    "    medsl_dfs[os.path.basename(filepath)[:-4]] = et.read_file(filepath).extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1.3.__ Gather Wikipedia data for comparison purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate wikipedia page titles\n",
    "pres_election = [('PRES', 'United States presidential election')]\n",
    "fed_elections = [('SEN', 'United States Senate election')]\n",
    "               # ('USH', United States House of Representatives election')]\n",
    "election_years_to_check = [2016, 2018]\n",
    "\n",
    "wiki_titles = []\n",
    "for yr in election_years_to_check:\n",
    "    generate_key = lambda yr, ekey: ekey + str(yr % 100)\n",
    "    generate_title = lambda yr, etype: str(yr) + ' ' + etype\n",
    "    \n",
    "    if yr % 4 == 0:\n",
    "        wiki_titles.append((generate_key(yr, pres_election[0][0]),\n",
    "                            generate_title(yr, pres_election[0][1])))\n",
    "        \n",
    "    [wiki_titles.append(((generate_key(yr, ekey) + '_' + st_abv),\n",
    "                         (generate_title(yr, etype) + ' in ' + st) ))\n",
    "         for ekey, etype in fed_elections\n",
    "         for st, st_abv in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather wikipedia page URLs\n",
    "wiki_urls = {}\n",
    "for wiki_title in wiki_titles:\n",
    "    key, title = wiki_title\n",
    "    try:\n",
    "        wiki_urls[key] = (title, wikipedia.page(title=title).url)\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print retrieved page URLs\n",
    "for wiki_key in wiki_urls:\n",
    "    title, url = wiki_urls[wiki_key]\n",
    "    print('{:10} : {}\\n\\t{}'.format(wiki_key, title, url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather wikipedia tabular election results\n",
    "wiki_tables = {}\n",
    "for wiki_key in wiki_urls:\n",
    "    try:\n",
    "        wiki_tables[wiki_key] = pd.read_html(wiki_urls[wiki_key][1])\n",
    "    except Exception as e:\n",
    "        print(\"Unable to gather Wikipedia tabular data:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display wikipedia tabular election data\n",
    "for wiki in wiki_tables:\n",
    "    print('================================================')\n",
    "    print('Wiki: {} '.format(wiki))\n",
    "    print('================================================')\n",
    "    \n",
    "    for i in range(len(wiki_tables[wiki])):\n",
    "        print('TABLE {}: ############################\\n{}\\n\\n\\n'.format(\n",
    "                i, wiki_tables[wiki][i].head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually gather found tabular election results\n",
    "wiki_dfs = {}\n",
    "\n",
    "# wiki_pres16_by_state = [{state_abv : table} for _______] # TODO\n",
    "\n",
    "# has to be done manually because every wikipedia page is different\n",
    "\n",
    "# wiki_dfs['PRES16_AL'] = wiki_pres16_by_state['AL']\n",
    "# wiki_dfs['PRES16_AK'] = wiki_pres16_by_state['AK']\n",
    "# wiki_dfs['PRES16_AZ'] = wiki_pres16_by_state['AZ']\n",
    "# wiki_dfs['PRES16_AR'] = wiki_pres16_by_state['AR']\n",
    "# wiki_dfs['PRES16_CA'] = wiki_pres16_by_state['CA']\n",
    "# wiki_dfs['PRES16_CO'] = wiki_pres16_by_state['CO']\n",
    "# wiki_dfs['PRES16_CT'] = wiki_pres16_by_state['CT']\n",
    "# wiki_dfs['PRES16_DE'] = wiki_pres16_by_state['DE']\n",
    "# wiki_dfs['PRES16_FL'] = wiki_pres16_by_state['FL']\n",
    "# wiki_dfs['PRES16_GA'] = wiki_pres16_by_state['GA']\n",
    "# wiki_dfs['PRES16_HI'] = wiki_pres16_by_state['HI']\n",
    "# wiki_dfs['PRES16_ID'] = wiki_pres16_by_state['ID']\n",
    "# wiki_dfs['PRES16_IL'] = wiki_pres16_by_state['IL']\n",
    "# wiki_dfs['PRES16_IN'] = wiki_pres16_by_state['IN']\n",
    "# wiki_dfs['PRES16_IA'] = wiki_pres16_by_state['IA']\n",
    "# wiki_dfs['PRES16_KS'] = wiki_pres16_by_state['KS']\n",
    "# wiki_dfs['PRES16_KY'] = wiki_pres16_by_state['KY']\n",
    "# wiki_dfs['PRES16_LA'] = wiki_pres16_by_state['LA']\n",
    "# wiki_dfs['PRES16_ME'] = wiki_pres16_by_state['ME']\n",
    "# wiki_dfs['PRES16_MD'] = wiki_pres16_by_state['MD']\n",
    "# wiki_dfs['PRES16_MA'] = wiki_pres16_by_state['MA']\n",
    "# wiki_dfs['PRES16_MI'] = wiki_pres16_by_state['MI']\n",
    "# wiki_dfs['PRES16_MN'] = wiki_pres16_by_state['MN']\n",
    "# wiki_dfs['PRES16_MS'] = wiki_pres16_by_state['MS']\n",
    "# wiki_dfs['PRES16_MO'] = wiki_pres16_by_state['MO']\n",
    "# wiki_dfs['PRES16_MT'] = wiki_pres16_by_state['MT']\n",
    "# wiki_dfs['PRES16_NE'] = wiki_pres16_by_state['NE']\n",
    "# wiki_dfs['PRES16_NV'] = wiki_pres16_by_state['NV']\n",
    "# wiki_dfs['PRES16_NH'] = wiki_pres16_by_state['NH']\n",
    "# wiki_dfs['PRES16_NJ'] = wiki_pres16_by_state['NJ']\n",
    "# wiki_dfs['PRES16_NM'] = wiki_pres16_by_state['NM']\n",
    "# wiki_dfs['PRES16_NY'] = wiki_pres16_by_state['NY']\n",
    "# wiki_dfs['PRES16_NC'] = wiki_pres16_by_state['NC']\n",
    "# wiki_dfs['PRES16_ND'] = wiki_pres16_by_state['ND']\n",
    "# wiki_dfs['PRES16_OH'] = wiki_pres16_by_state['OH']\n",
    "# wiki_dfs['PRES16_OK'] = wiki_pres16_by_state['OK']\n",
    "# wiki_dfs['PRES16_OR'] = wiki_pres16_by_state['OR']\n",
    "# wiki_dfs['PRES16_PA'] = wiki_pres16_by_state['PA']\n",
    "# wiki_dfs['PRES16_RI'] = wiki_pres16_by_state['RI']\n",
    "# wiki_dfs['PRES16_SC'] = wiki_pres16_by_state['SC']\n",
    "# wiki_dfs['PRES16_SD'] = wiki_pres16_by_state['SD']\n",
    "# wiki_dfs['PRES16_TN'] = wiki_pres16_by_state['TN']\n",
    "# wiki_dfs['PRES16_TX'] = wiki_pres16_by_state['TX']\n",
    "# wiki_dfs['PRES16_UT'] = wiki_pres16_by_state['UT']\n",
    "# wiki_dfs['PRES16_VT'] = wiki_pres16_by_state['VT']\n",
    "# wiki_dfs['PRES16_VA'] = wiki_pres16_by_state['VA']\n",
    "# wiki_dfs['PRES16_WA'] = wiki_pres16_by_state['WA']\n",
    "# wiki_dfs['PRES16_WV'] = wiki_pres16_by_state['WV']\n",
    "# wiki_dfs['PRES16_WI'] = wiki_pres16_by_state['WI']\n",
    "# wiki_dfs['PRES16_WY'] = wiki_pres16_by_state['WY']\n",
    "\n",
    "wiki_dfs['SEN16_AL'] = wiki_tables['SEN16_AL'][19]\n",
    "wiki_dfs['SEN16_AK'] = wiki_tables['SEN16_AK'][20]\n",
    "wiki_dfs['SEN16_AZ'] = wiki_tables['SEN16_AZ'][40]\n",
    "# wiki_dfs['SEN16_AR'] =\n",
    "# wiki_dfs['SEN16_CA'] =\n",
    "# wiki_dfs['SEN16_CO'] = \n",
    "# wiki_dfs['SEN16_CT'] = \n",
    "# wiki_dfs['SEN16_DE'] = \n",
    "# wiki_dfs['SEN16_FL'] = \n",
    "# wiki_dfs['SEN16_GA'] = \n",
    "# wiki_dfs['SEN16_HI'] = \n",
    "# wiki_dfs['SEN16_ID'] = \n",
    "# wiki_dfs['SEN16_IL'] = \n",
    "# wiki_dfs['SEN16_IN'] = \n",
    "# wiki_dfs['SEN16_IA'] = \n",
    "# wiki_dfs['SEN16_KS'] = \n",
    "# wiki_dfs['SEN16_KY'] = \n",
    "# wiki_dfs['SEN16_LA'] = \n",
    "# wiki_dfs['SEN16_ME'] = \n",
    "# wiki_dfs['SEN16_MD'] = \n",
    "# wiki_dfs['SEN16_MA'] = \n",
    "# wiki_dfs['SEN16_MI'] = \n",
    "# wiki_dfs['SEN16_MN'] = \n",
    "# wiki_dfs['SEN16_MS'] = \n",
    "# wiki_dfs['SEN16_MO'] = \n",
    "# wiki_dfs['SEN16_MT'] = \n",
    "# wiki_dfs['SEN16_NE'] = \n",
    "# wiki_dfs['SEN16_NV'] = \n",
    "# wiki_dfs['SEN16_NH'] = \n",
    "# wiki_dfs['SEN16_NJ'] = \n",
    "# wiki_dfs['SEN16_NM'] = \n",
    "# wiki_dfs['SEN16_NY'] = \n",
    "# wiki_dfs['SEN16_NC'] = \n",
    "# wiki_dfs['SEN16_ND'] = \n",
    "# wiki_dfs['SEN16_OH'] = \n",
    "# wiki_dfs['SEN16_OK'] = \n",
    "# wiki_dfs['SEN16_OR'] = \n",
    "# wiki_dfs['SEN16_PA'] = \n",
    "# wiki_dfs['SEN16_RI'] = \n",
    "# wiki_dfs['SEN16_SC'] = \n",
    "# wiki_dfs['SEN16_SD'] = \n",
    "# wiki_dfs['SEN16_TN'] = \n",
    "# wiki_dfs['SEN16_TX'] = \n",
    "# wiki_dfs['SEN16_UT'] = \n",
    "# wiki_dfs['SEN16_VT'] = \n",
    "# wiki_dfs['SEN16_VA'] = \n",
    "# wiki_dfs['SEN16_WA'] = \n",
    "# wiki_dfs['SEN16_WV'] = \n",
    "# wiki_dfs['SEN16_WI'] = \n",
    "# wiki_dfs['SEN16_WY'] = \n",
    "\n",
    "wiki_dfs['SEN18_AZ'] = wiki_tables['SEN18_AZ'][40]\n",
    "# wiki_dfs['SEN18_AR'] = \n",
    "# wiki_dfs['SEN18_CA'] = \n",
    "# wiki_dfs['SEN18_CO'] = \n",
    "# wiki_dfs['SEN18_CT'] = \n",
    "# wiki_dfs['SEN18_DE'] = \n",
    "# wiki_dfs['SEN18_FL'] = \n",
    "# wiki_dfs['SEN18_GA'] = \n",
    "# wiki_dfs['SEN18_HI'] = \n",
    "# wiki_dfs['SEN18_ID'] = \n",
    "# wiki_dfs['SEN18_IL'] = \n",
    "# wiki_dfs['SEN18_IN'] = \n",
    "# wiki_dfs['SEN18_IA'] = \n",
    "# wiki_dfs['SEN18_KS'] = \n",
    "# wiki_dfs['SEN18_KY'] = \n",
    "# wiki_dfs['SEN18_LA'] = \n",
    "# wiki_dfs['SEN18_ME'] = \n",
    "# wiki_dfs['SEN18_MD'] = \n",
    "# wiki_dfs['SEN18_MA'] = \n",
    "# wiki_dfs['SEN18_MI'] = \n",
    "# wiki_dfs['SEN18_MN'] = \n",
    "# wiki_dfs['SEN18_MS'] = \n",
    "# wiki_dfs['SEN18_MO'] = \n",
    "# wiki_dfs['SEN18_MT'] = \n",
    "# wiki_dfs['SEN18_NE'] = \n",
    "# wiki_dfs['SEN18_NV'] = \n",
    "# wiki_dfs['SEN18_NH'] = \n",
    "# wiki_dfs['SEN18_NJ'] = \n",
    "# wiki_dfs['SEN18_NM'] = \n",
    "# wiki_dfs['SEN18_NY'] = \n",
    "# wiki_dfs['SEN18_NC'] = \n",
    "# wiki_dfs['SEN18_ND'] = \n",
    "# wiki_dfs['SEN18_OH'] = \n",
    "# wiki_dfs['SEN18_OK'] = \n",
    "# wiki_dfs['SEN18_OR'] = \n",
    "# wiki_dfs['SEN18_PA'] = \n",
    "# wiki_dfs['SEN18_RI'] = \n",
    "# wiki_dfs['SEN18_SC'] = \n",
    "# wiki_dfs['SEN18_SD'] = \n",
    "# wiki_dfs['SEN18_TN'] = \n",
    "# wiki_dfs['SEN18_TX'] = \n",
    "# wiki_dfs['SEN18_UT'] = \n",
    "# wiki_dfs['SEN18_VT'] = \n",
    "# wiki_dfs['SEN18_VA'] = \n",
    "# wiki_dfs['SEN18_WA'] = \n",
    "# wiki_dfs['SEN18_WV'] = \n",
    "# wiki_dfs['SEN18_WI'] = \n",
    "# wiki_dfs['SEN18_WY'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Coming Soon] __Step 1.4.__ Gather Ballotpedia data for comparison purposes\n",
    "\n",
    "*Note:* Depending on response to API access, this step may need to be done manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Data wrangling\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2.1.__ Wrangle MEDSL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_medsl(medsl_dfs_dict):\n",
    "    for df in medsl_dfs_dict:\n",
    "        medsl_pvt = medsl_dfs_dict[df].pivot_table(index='precinct',\n",
    "                                                   columns=['office', 'party'],\n",
    "                                                   values='votes')\n",
    "        medsl_pvt.columns = [' '.join(col).strip() for col in medsl_pvt.columns.values]\n",
    "        medsl_dfs_dict[df] = et.ExtractTable(medsl_pvt).extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View available data\n",
    "for df in medsl_dfs:\n",
    "    print('--------{}--------'.format(df))\n",
    "    print(medsl_dfs[df].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant election data\n",
    "medsl_18_dfs = {}\n",
    "medsl_pres16_dfs = {}\n",
    "medsl_sen16_dfs = {}\n",
    "medsl_ush16_dfs = {}\n",
    "\n",
    "for state in states:\n",
    "    st, st_abv = state\n",
    "    try:\n",
    "        medsl_18_dfs[st] = et.ExtractTable(medsl_dfs['precinct_2018'], column='state', value=st).extract()\n",
    "    except Exception as e:\n",
    "        print('Missing data in medsl_18:', e)\n",
    "    \n",
    "    try:\n",
    "        medsl_pres16_dfs[st] = et.ExtractTable(medsl_dfs['2016-precinct-president'], \n",
    "                                               column='state', value=st).extract()\n",
    "    except Exception as e:\n",
    "        print('Missing data in medsl_pres16:', e)\n",
    "    \n",
    "    try:\n",
    "        medsl_sen16_dfs[st] = et.ExtractTable(medsl_dfs['2016-precinct-senate'], \n",
    "                                               column='state', value=st).extract()\n",
    "    except Exception as e:\n",
    "        print('Missing data in medsl_sen16:', e)\n",
    "    \n",
    "    try:\n",
    "        medsl_ush16_dfs[st] = et.ExtractTable(medsl_dfs['2016-precinct-house'], \n",
    "                                               column='state', value=st).extract()\n",
    "    except Exception as e:\n",
    "        print('Missing data in medsl_ush16:', e)\n",
    "\n",
    "        \n",
    "pivot_medsl(medsl_18_dfs)\n",
    "pivot_medsl(medsl_pres16_dfs)\n",
    "pivot_medsl(medsl_sen16_dfs)\n",
    "pivot_medsl(medsl_ush16_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Data standardization check of ``mggg-states``\n",
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3.1__ Generate naming conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naming_convention.json') as json_file:\n",
    "    standards_raw = json.load(json_file)\n",
    "    \n",
    "offices = dm.get_keys_by_category(standards_raw, 'offices')\n",
    "parties = dm.get_keys_by_category(standards_raw, 'parties')\n",
    "counts = dm.get_keys_by_category(standards_raw, 'counts')\n",
    "others = dm.get_keys_by_category(standards_raw, \n",
    "            ['geographies', 'demographics', 'districts', 'other'])\n",
    "\n",
    "elections = [office + format(year, '02') + party \n",
    "                 for office in offices\n",
    "                 for year in range(0, 21)\n",
    "                 for party in parties \n",
    "                 if not (office == 'PRES' and year % 4 != 0)]\n",
    "\n",
    "counts = [count + format(year, '02') for count in counts \n",
    "                                     for year in range(0, 20)]\n",
    "\n",
    "standards = elections + counts + others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3.2.__ Check ``mggg-states`` data compliance with naming conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_check = {}\n",
    "\n",
    "for gdf in mggg_gdfs:\n",
    "      naming_check[gdf] = dq.compare_column_names(mggg_gdfs[gdf], standards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_columns = {}\n",
    "\n",
    "# Print results\n",
    "for gdf in naming_check:\n",
    "    print('=========================================')\n",
    "    print('Dataset: {}'.format(gdf))\n",
    "    print('=========================================')\n",
    "    \n",
    "    (matches, diffs) = naming_check[gdf]\n",
    "    matched_columns[gdf] = matches\n",
    "    \n",
    "    diffs = list(diffs)\n",
    "    diffs.sort()\n",
    "    \n",
    "    print('Discrepancies from naming convention:')\n",
    "    print(diffs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Compare ``mggg-states`` data with external sources\n",
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_mggg_states = []\n",
    "for state in states:\n",
    "    state_name, state_abv = state\n",
    "    \n",
    "    # messy name matching because file naming isn't standardized\n",
    "    mggg_gdf_names = [gdf_name for gdf_name in list(mggg_gdfs) \n",
    "                               if gdf_name.startswith(state_abv.lower() + '_') or\n",
    "                                  gdf_name.startswith(state_abv + '_') or\n",
    "                                  gdf_name.startswith(state_name.lower() + '_') or\n",
    "                                  gdf_name.startswith(state_name.upper() + '_') or\n",
    "                                  gdf_name.startswith(state_name + '_') or\n",
    "                                  gdf_name.startswith(\n",
    "                                      state_name.replace(' ', '_').lower() + '_') or\n",
    "                                  gdf_name.startswith(\n",
    "                                      state_name.replace(' ', '_').upper() + '_') or\n",
    "                                  gdf_name.startswith(\n",
    "                                      state_name.replace(' ', '_') + '_')]\n",
    "                      \n",
    "    available_mggg_states.append((state_name, mggg_gdf_names))\n",
    "    \n",
    "available_mggg_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View available columns\n",
    "for state in available_mggg_states:\n",
    "    st, gdf_names = state\n",
    "    if gdf_names:\n",
    "        print('======== {} ========'.format(st))\n",
    "        for name in gdf_names:\n",
    "            print('{} --------'.format(name))\n",
    "            print(sorted(list(matched_columns[name])))\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4.1.__ Compare against MEDSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Naming Convention Translations between MGGG and MEDSL\n",
    "pres16_cols = [\n",
    "    ('PRES16D', 'US President democratic'), \n",
    "    ('PRES16G', 'US President green'), \n",
    "    ('PRES16L', 'US President libertarian'), \n",
    "    ('PRES16R', 'US President republican')\n",
    "]\n",
    "\n",
    "sen16_cols = [\n",
    "    ('SEN16D', 'US Senate democrat'),\n",
    "    ('SEN16G', 'US Senate green'),\n",
    "    ('SEN16L', 'US Senate libertarian'),\n",
    "    ('SEN16R', 'US Senate republican')\n",
    "]\n",
    "\n",
    "ush16_cols = [\n",
    "    ('USH16D', 'US House democratic'),\n",
    "    ('USH16G', 'US House green'),\n",
    "    ('USH16L', 'US House libertarian'),\n",
    "    ('USH16R', 'US House republican')\n",
    "]\n",
    "\n",
    "fed18_cols = [\n",
    "    ('SEN18D', 'US Senate democratic'),\n",
    "    ('SEN18G', 'US Senate green'),\n",
    "    ('SEN18L', 'US Senate libertarian'),\n",
    "    ('SEN18R', 'US Senate republican'),\n",
    "    ('USH18D', 'US House democrat'),\n",
    "    ('USH18G', 'US House green'),\n",
    "    ('USH18L', 'US House libertarian'),\n",
    "    ('USH18R', 'US House republican')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_compare(bulk_results: Dict[str, List[Tuple[Hashable, Any]]], \n",
    "                 st: str, \n",
    "                 mggg_names: List[str], \n",
    "                 medsls: List[pd.DataFrame], \n",
    "                 cols: Tuple[str, str]):\n",
    "    \"\"\"\n",
    "    Returns, by reference, a dict containing state names as keys and a value of a \n",
    "    dict containing mggg_gdf names as keys and the results of dm.compare_column_sums\n",
    "    as values.\n",
    "    \n",
    "    \"\"\"\n",
    "    if st in medsls:\n",
    "        x = []\n",
    "        y = []\n",
    "        for mggg_col, medsl_col in cols:\n",
    "            if (mggg_col in list(mggg_gdfs[gdf_name]) and \n",
    "                medsl_col in list(medsls[st])):\n",
    "                x.append(mggg_col)\n",
    "                y.append(medsl_col)\n",
    "                \n",
    "        if len(x) != 0:\n",
    "            try:\n",
    "                bulk_results[st] += dq.compare_column_sums(\n",
    "                                        mggg_gdfs[gdf_name], medsls[st], x, y)\n",
    "            except Exception as e:\n",
    "                try:\n",
    "                    bulk_results[st] = dq.compare_column_sums(\n",
    "                                        mggg_gdfs[gdf_name], medsls[st], x, y)\n",
    "                except Exception as z:\n",
    "                    print(\"Unable to compare in {}. {}. Likely due to dtype incompatibility.\".format(st, z))\n",
    "\n",
    "results = {}\n",
    "\n",
    "for state in available_mggg_states:\n",
    "    st, mggg_names = state\n",
    "    \n",
    "    if mggg_names:\n",
    "        bulk_compare(results, st, mggg_names, medsl_pres16_dfs, pres16_cols)\n",
    "        bulk_compare(results, st, mggg_names, medsl_sen16_dfs, sen16_cols)\n",
    "        bulk_compare(results, st, mggg_names, medsl_ush16_dfs, ush16_cols)\n",
    "        bulk_compare(results, st, mggg_names, medsl_18_dfs, fed18_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison results\n",
    "print(\"============================================================================\")\n",
    "print(\"Results of State-level Aggregation comparisons between mggg-states and MEDSL\")\n",
    "print(\"============================================================================\")\n",
    "print()\n",
    "print('{:37} : {}'.format('mggg-states column [vs] MEDSL column', 'difference in sums'))\n",
    "print('----------------------------------------------------------------------------')\n",
    "\n",
    "for st in results:\n",
    "    print(\"######## {} ########\".format(st))\n",
    "    for col_v_col, diff in results[st]:\n",
    "        print('{:37} : {}'.format(col_v_col, diff))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Don't panic just yet -- potential explanations for differences:__\n",
    "\n",
    "- MGGG data may have columns with mixed datatypes. Some columns contain strings, when they should contain numbers\n",
    "- Some shapefiles use VTDs and towns, which may differ from precincts\n",
    "- Undercounts may be the result of excluding absentee ballots in the count\n",
    "- Sums should be spot-checked manually in addition to the automated checks\n",
    "- The comparisons are conducted on the intersection between mggg-states and MEDSL datasets and are limited to 2016 and 2018 data\n",
    "- I also haven't rigourously examined the MEDSL data :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Coming Soon] __Step 4.2.__ Compare against Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Coming Soon] __Step 4.3.__ Compare against Ballotpedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4.4__ Data summation for manual checks against external sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print MGGG summation results\n",
    "print(\"============================================================================\")\n",
    "print(\"Results of State-level Aggregations - mggg-states\")\n",
    "print(\"============================================================================\")\n",
    "print()\n",
    "print('{:11} : {:10}\\t\\t{}'.format('mggg-states column', 'sum', 'dtype'))\n",
    "print('----------------------------------------------------------------------------')\n",
    "\n",
    "for st, gdf_name in available_mggg_states:\n",
    "    print(\"######## {} : {} ########\".format(st, gdf_name))\n",
    "    \n",
    "    cols_to_sum = [col for col in matched_columns[gdf_name] if col != 'geometry']\n",
    "    col_sums = dq.sum_column_values(mggg_gdfs[gdf_name], cols_to_sum)\n",
    "    \n",
    "    for result in col_sums:\n",
    "        col_name, col_sum = result\n",
    "        print('{:11} : {:10}\\t\\t{}'.format(col_name, col_sum, str(type(col_sum))))\n",
    "    print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print MEDSL summation results\n",
    "def print_medsl_sums(st, medsl_name, medsls):\n",
    "    if st in medsls:\n",
    "        print(\"######## {} : {} ########\".format(st, medsl_name))\n",
    "        \n",
    "        cols_to_sum = [col for col in list(medsls[st]) if col != 'geometry']\n",
    "        try:\n",
    "            col_sums = dq.sum_column_values(medsls[st], cols_to_sum)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        for result in col_sums:\n",
    "            col_name, col_sum = result\n",
    "            print('{:70} : {}\\t{}'.format(col_name, col_sum, str(type(col_sum))))\n",
    "\n",
    "        print('\\n\\n')\n",
    "\n",
    "        \n",
    "print(\"============================================================================\")\n",
    "print(\"Results of State-level Aggregations - MEDSL\")\n",
    "print(\"============================================================================\")\n",
    "print()\n",
    "print('{:70} : {}\\t{}'.format('MEDSL column', 'sum', 'dtype'))\n",
    "print('----------------------------------------------------------------------------')\n",
    "\n",
    "for st in states:\n",
    "    print_medsl_sums(st, 'MEDSL 2018', medsl_18_dfs)\n",
    "    print_medsl_sums(st, 'MEDSL PRES16', medsl_pres16_dfs)\n",
    "    print_medsl_sums(st, 'MEDSL SEN16', medsl_sen16_dfs)\n",
    "    print_medsl_sums(st, 'MEDSL USH16', medsl_ush16_dfs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Check topological soundness of ``mggg-states`` data\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "__Step 5.1.__ Check for empty geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topological_warnings = []\n",
    "for gdf in mggg_gdfs:\n",
    "    if any(mggg_gdfs[gdf]['geometry'].isna()):\n",
    "        topological_warnings.append('{} has missing geometries.'.format(gdf))\n",
    "        \n",
    "    if any(mggg_gdfs[gdf]['geometry'].is_empty):\n",
    "        topological_warnings.append('{} has empty geometries.')\n",
    "\n",
    "if len(topological_warnings) == 0:\n",
    "    print(\"All topologies are sound.\")\n",
    "else:\n",
    "    [print(msg) for msg in topological_warnings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6. Cleanup\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove cloned repos\n",
    "# dm.remove_repos('qafiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete output dump directory\n",
    "# !echo y | rm -rf ./qafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uninstall installed python packages\n",
    "# !echo y | pip3 uninstall numpy\n",
    "# !echo y | pip3 uninstall pandas\n",
    "# !echo y | pip3 uninstall geopandas\n",
    "# !echo y | pip3 uninstall wikipedia\n",
    "\n",
    "# !echo y | pip3 uninstall gdutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reset Jupyter Notebook IPython Kernel\n",
    "# from IPython.core.display import HTML\n",
    "# HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "-------------\n",
    "\n",
    "__Data Standardization__\n",
    "\n",
    "- Manually evaluate column naming discrepancies to determine if changes are needed\n",
    "- Manually evaluate column datatypes to determine if changes are needed\n",
    "\n",
    "__Data Comparison__\n",
    "\n",
    "- Manually investigate large differences found through comparing ``mggg-states`` data with external sources (e.g. Are absentee ballots counted? Are the precinct counts accurate? etc.) \n",
    "- For more accurate comparisons, compare ``mggg-states`` data with those in each States' Secretary of State website\n",
    "\n",
    "__Topological Soundness__\n",
    "\n",
    "- Manually examine shapefiles for gaps and overlaps. *Note:* although gaps and overlaps are not necessarily indicators of inaccurate data (because some counties have precinct islands), they do mean that the data cannot be for chain runs. \n",
    "\n",
    "__Data Documentation__\n",
    "\n",
    "- Do the READMEs provide data sources?\n",
    "- Do the READMEs describe what aggregation/disaggregation processes were used?\n",
    "- Do the READMEs discuss discrepancies/caveats in the data?\n",
    "- Do the READMEs provide scripts used and/or discuss the data wrangling/processing process?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
